{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Summary\n",
    "This is an updated version of my previous notebook. Algorithms used in this notebook are **LogisticRegression**, **RandomForest**, and **GradientBoosting**. I got 0.79425 score from this notebook. This notebook consists of:\n",
    "1. Feature Engineering\n",
    "2. Feature Preprocessing for ML\n",
    "3. Classification Model Check\n",
    "4. Predictions\n",
    "\n",
    "References:\n",
    "1. https://www.kaggle.com/startupsci/titanic-data-science-solutions\n",
    "2. https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df_new = pd.read_csv('test.csv')\n",
    "df = df.drop(columns='PassengerId')\n",
    "df_new = df_new.drop(columns='PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 11 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Survived  891 non-null    int64  \n",
      " 1   Pclass    891 non-null    int64  \n",
      " 2   Name      891 non-null    object \n",
      " 3   Sex       891 non-null    object \n",
      " 4   Age       714 non-null    float64\n",
      " 5   SibSp     891 non-null    int64  \n",
      " 6   Parch     891 non-null    int64  \n",
      " 7   Ticket    891 non-null    object \n",
      " 8   Fare      891 non-null    float64\n",
      " 9   Cabin     204 non-null    object \n",
      " 10  Embarked  889 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 76.7+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 10 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Pclass    418 non-null    int64  \n",
      " 1   Name      418 non-null    object \n",
      " 2   Sex       418 non-null    object \n",
      " 3   Age       332 non-null    float64\n",
      " 4   SibSp     418 non-null    int64  \n",
      " 5   Parch     418 non-null    int64  \n",
      " 6   Ticket    418 non-null    object \n",
      " 7   Fare      417 non-null    float64\n",
      " 8   Cabin     91 non-null     object \n",
      " 9   Embarked  418 non-null    object \n",
      "dtypes: float64(2), int64(3), object(5)\n",
      "memory usage: 32.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "print(df_new.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "## Extract Titles From Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Extracted!\n",
      "Name Dropped!\n"
     ]
    }
   ],
   "source": [
    "# Extract titles from 'Name'\n",
    "df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "df_new['Title'] = df_new.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "mapping = {'Mr' : 'Mr', 'Dr' : 'Dr', 'Miss' : 'Ms', 'Ms' : 'Ms', 'Mrs' : 'Mrs', 'Master' : 'Master', 'Rev' : 'Rev', 'Miss' : 'Ms', 'Mlle' : 'Ms', 'Mme' : 'Mrs',\n",
    "'Major' : 'Mill', 'Capt' : 'Mill', 'Col' : 'Mill', 'Lady' : 'Hon', 'Jonkheer' : 'Hon', 'Sir' : 'Hon', 'Don' : 'Hon', 'Countess' : 'Hon', 'Dona' : 'Hon'}\n",
    "\n",
    "df.Title = df.Title.map(mapping)\n",
    "df_new.Title = df_new.Title.map(mapping)\n",
    "print('Title Extracted!')\n",
    "df = df.drop(columns='Name')\n",
    "df_new = df_new.drop(columns='Name')\n",
    "print('Name Dropped!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Family (SibSp and Parch)\n",
    "I found out that passengers who travel with family are more likely to survive by using the following scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alone or Not\n",
    "def alone_func(family):\n",
    "    alone = []\n",
    "    for x in family:\n",
    "        if x == 0:\n",
    "            alone.append(int(1))\n",
    "        else:\n",
    "            alone.append(int(0))\n",
    "    return(alone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alone created!\n"
     ]
    }
   ],
   "source": [
    "df['Alone'] = alone_func(df.SibSp + df.Parch)\n",
    "df_new['Alone'] = alone_func(df_new.SibSp + df_new.Parch)\n",
    "print('Alone created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alone  0 (354 data)\n",
      "0    0.49435\n",
      "1    0.50565\n",
      "Name: Survived, dtype: float64 \n",
      "\n",
      "Alone  1 (537 data)\n",
      "0    0.696462\n",
      "1    0.303538\n",
      "Name: Survived, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for alone in sorted(df.Alone.unique()):\n",
    "    print('Alone ', alone, '({} data)'.format(len(df.Survived[df.Alone == alone])))\n",
    "    print(df.Survived[df.Alone == alone].value_counts(normalize=True).sort_index(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above, it is clear that most passengers who travel alone are dead. I also removed SibSp and Parch because they are correlated with Alone feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SibSp and Parch dropped!\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=['SibSp', 'Parch'])\n",
    "df_new = df_new.drop(columns=['SibSp', 'Parch'])\n",
    "print('SibSp and Parch dropped!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ticket\n",
    "I tried to categorize Ticket but unfortunately there are some ticket categories in the test set that are not available in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def ticket_func(ticket_data):\n",
    "    \"\"\"Split ticket to a dataframe with TicketCat and TicketNo\"\"\"\n",
    "    ticket_data = ticket_data.str.upper() # uppercase all entries\n",
    "    ticket_data = ticket_data.str.replace('STON', 'SOTON') # fix alleged typos\n",
    "    ticket_data = ticket_data.str.split(n=1, expand=True)\n",
    "    for i, entry in ticket_data.iterrows():\n",
    "        if ticket_data[0][i].isdecimal():\n",
    "            ticket_data[1][i] = ticket_data[0][i]\n",
    "            ticket_data[0][i] = 'no'\n",
    "        ticket_data[0][i] = ''.join(re.findall('[a-zA-Z0-9/]', ticket_data[0][i])) # match alphanumerical and slash (/)\n",
    "    ticket_data.columns = ['TicketCat', 'TicketNo']\n",
    "    ticket_data['TicketNo'] = ticket_data['TicketNo'].fillna(0)\n",
    "    return(ticket_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  TicketCat TicketNo\n",
      "0       A/5    21171\n",
      "1        PC    17599\n",
      "2  SOTON/O2  3101282\n",
      "3        no   113803\n",
      "4        no   373450\n",
      "  TicketCat TicketNo\n",
      "0        no   330911\n",
      "1        no   363272\n",
      "2        no   240276\n",
      "3        no   315154\n",
      "4        no  3101298\n"
     ]
    }
   ],
   "source": [
    "df[['TicketCat', 'TicketNo']] = ticket_func(df.Ticket)\n",
    "df_new[['TicketCat', 'TicketNo']] = ticket_func(df_new.Ticket)\n",
    "print(df[['TicketCat', 'TicketNo']].head())\n",
    "print(df_new[['TicketCat', 'TicketNo']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'AQ/3', 'AQ/4', 'LP', 'SC/A3']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check TicketCat in test set which are not available in training set\n",
    "sorted([x for x in df_new.TicketCat.unique() if not x in df.TicketCat.unique()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the category differences in test set and training set, I dropped Ticket features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticket dropped!\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=['TicketCat', 'TicketNo', 'Ticket'])\n",
    "df_new = df_new.drop(columns=['TicketCat', 'TicketNo', 'Ticket'])\n",
    "print('Ticket dropped!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cabin_func(cabin_data):\n",
    "    cabin_data = cabin_data.fillna('N') # fill nan with N\n",
    "    cabin_data = cabin_data.str[0]\n",
    "    cabin_data = cabin_data.str.replace('N', 'NO DATA')\n",
    "    return(cabin_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Cabin = cabin_func(df.Cabin)\n",
    "df_new.Cabin = cabin_func(df_new.Cabin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'NO DATA', 'T']\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'NO DATA']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(df.Cabin.unique()))\n",
    "print(sorted(df_new.Cabin.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cabin  A (15 data)\n",
      "0    0.533333\n",
      "1    0.466667\n",
      "Name: Survived, dtype: float64 \n",
      "\n",
      "Cabin  B (47 data)\n",
      "0    0.255319\n",
      "1    0.744681\n",
      "Name: Survived, dtype: float64 \n",
      "\n",
      "Cabin  C (59 data)\n",
      "0    0.40678\n",
      "1    0.59322\n",
      "Name: Survived, dtype: float64 \n",
      "\n",
      "Cabin  D (33 data)\n",
      "0    0.242424\n",
      "1    0.757576\n",
      "Name: Survived, dtype: float64 \n",
      "\n",
      "Cabin  E (32 data)\n",
      "0    0.25\n",
      "1    0.75\n",
      "Name: Survived, dtype: float64 \n",
      "\n",
      "Cabin  F (13 data)\n",
      "0    0.384615\n",
      "1    0.615385\n",
      "Name: Survived, dtype: float64 \n",
      "\n",
      "Cabin  G (4 data)\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: Survived, dtype: float64 \n",
      "\n",
      "Cabin  NO DATA (687 data)\n",
      "0    0.700146\n",
      "1    0.299854\n",
      "Name: Survived, dtype: float64 \n",
      "\n",
      "Cabin  T (1 data)\n",
      "0    1.0\n",
      "Name: Survived, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cabin in sorted(df.Cabin.unique()):\n",
    "    print('Cabin ', cabin, '({} data)'.format(len(df.Survived[df.Cabin == cabin])))\n",
    "    print(df.Survived[df.Cabin == cabin].value_counts(normalize=True).sort_index(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look from the output above, the Cabin category might have some impact to the survival probability. However, there are so many missing data in this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data in Cabin = 77.10% of total training data\n"
     ]
    }
   ],
   "source": [
    "print('Missing data in Cabin = {:.2f}% of total training data'.format(len(df[df.Cabin == 'NO DATA'])/len(df)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null values in the Cabin does not give any information (I think the data is simply missing) therefore Cabin feature is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cabin Dropped!\n"
     ]
    }
   ],
   "source": [
    "# Cabin\n",
    "df = df.drop(columns=['Cabin'])\n",
    "df_new = df_new.drop(columns=['Cabin'])\n",
    "print('Cabin Dropped!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age\n",
    "I binned the Age feature into 5 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age = df.Age.fillna(df.Age.mean()) # impute before binning\n",
    "df_new.Age = df_new.Age.fillna(df.Age.mean()) # impute before binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize Age Data (Binning)\n",
    "def bin_age(age_data):\n",
    "    age_category = []\n",
    "    for age in age_data:\n",
    "        if age <= 16:\n",
    "            age_category.append('A')\n",
    "        elif (age > 16) & (age <= 32):\n",
    "            age_category.append('B')\n",
    "        elif (age > 32) & (age <= 48):\n",
    "            age_category.append('C')\n",
    "        elif (age > 48) & (age <= 64):\n",
    "            age_category.append('D')\n",
    "        else:\n",
    "            age_category.append('E')\n",
    "    return age_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgeCategory created!\n"
     ]
    }
   ],
   "source": [
    "df['AgeCategory'] = bin_age(df.Age)\n",
    "df_new['AgeCategory'] = bin_age(df_new.Age)\n",
    "print('AgeCategory created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Category  A (100 data)\n",
      "0    0.45\n",
      "1    0.55\n",
      "Name: Survived, dtype: float64 \n",
      "\n",
      "Age Category  B (523 data)\n",
      "0    0.655832\n",
      "1    0.344168\n",
      "Name: Survived, dtype: float64 \n",
      "\n",
      "Age Category  C (188 data)\n",
      "0    0.595745\n",
      "1    0.404255\n",
      "Name: Survived, dtype: float64 \n",
      "\n",
      "Age Category  D (69 data)\n",
      "0    0.565217\n",
      "1    0.434783\n",
      "Name: Survived, dtype: float64 \n",
      "\n",
      "Age Category  E (11 data)\n",
      "0    0.909091\n",
      "1    0.090909\n",
      "Name: Survived, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cat in sorted(df.AgeCategory.unique()):\n",
    "    print('Age Category ', cat, '({} data)'.format(len(df.Survived[df.AgeCategory == cat])))\n",
    "    print(df.Survived[df.AgeCategory == cat].value_counts(normalize=True).sort_index(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows significant differences in survival rate for each AgeCategory therefore Age column will no longer be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age dropped!\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns='Age')\n",
    "df_new = df_new.drop(columns='Age')\n",
    "print('Age dropped!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Missing Values\n",
    "In this notebook, I simply imputed the missing values with mean (for numerical) or most frequent value (for categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Survived     891 non-null    int64  \n",
      " 1   Pclass       891 non-null    int64  \n",
      " 2   Sex          891 non-null    object \n",
      " 3   Fare         891 non-null    float64\n",
      " 4   Embarked     891 non-null    object \n",
      " 5   Title        891 non-null    object \n",
      " 6   Alone        891 non-null    int64  \n",
      " 7   AgeCategory  891 non-null    object \n",
      "dtypes: float64(1), int64(3), object(4)\n",
      "memory usage: 55.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Pclass       418 non-null    int64  \n",
      " 1   Sex          418 non-null    object \n",
      " 2   Fare         418 non-null    float64\n",
      " 3   Embarked     418 non-null    object \n",
      " 4   Title        418 non-null    object \n",
      " 5   Alone        418 non-null    int64  \n",
      " 6   AgeCategory  418 non-null    object \n",
      "dtypes: float64(1), int64(2), object(4)\n",
      "memory usage: 23.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Impute\n",
    "df.Embarked = df.Embarked.fillna(df.Embarked.value_counts().index[0])\n",
    "df_new.Fare = df_new.Fare.fillna(df_new.Fare.mean())\n",
    "print(df.info())\n",
    "print(df_new.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preprocessing for ML\n",
    "This chapter includes class balance check, convert categorical with OneHotEncoder, and validation set split (30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.616162\n",
      "1    0.383838\n",
      "Name: Survived, dtype: float64\n",
      "0    549\n",
      "1    342\n",
      "Name: Survived, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check Class\n",
    "print(df.Survived.value_counts(normalize=True))\n",
    "print(df.Survived.value_counts())\n",
    "df.Survived.value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are well balanced with 60-40 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features:\n",
      "(891, 7)\n",
      "\n",
      "Test features:\n",
      "(418, 7)\n",
      "\n",
      "Target:\n",
      "(891,)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns='Survived')\n",
    "X_pred = df_new\n",
    "y = df.Survived\n",
    "print('Training features:')\n",
    "print(X.shape)\n",
    "print('\\nTest features:')\n",
    "print(X_pred.shape)\n",
    "print('\\nTarget:')\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features:\n",
      "(891, 21)\n",
      "\n",
      "Test features:\n",
      "(418, 21)\n",
      "\n",
      "Target:\n",
      "(891,)\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "X_categorical = X.select_dtypes(object)\n",
    "X_categorical = encoder.fit_transform(X_categorical)\n",
    "X_pred_categorical = X_pred.select_dtypes(object)\n",
    "X_pred_categorical = encoder.transform(X_pred_categorical)\n",
    "\n",
    "X_numerical = X.select_dtypes(exclude=object).to_numpy()\n",
    "X_pred_numerical = X_pred.select_dtypes(exclude=object).to_numpy()\n",
    "\n",
    "X = np.concatenate((X_categorical, X_numerical), axis=1)\n",
    "X_pred = np.concatenate((X_pred_categorical, X_pred_numerical), axis=1)\n",
    "\n",
    "print('Training features:')\n",
    "print(X.shape)\n",
    "print('\\nTest features:')\n",
    "print(X_pred.shape)\n",
    "print('\\nTarget:')\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set (30%) ready!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print('Validation set (30%) ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model Check\n",
    "I used **LogisticRegression, RandomForest, and GradientBoosting** methods. I printed out validation test set classification report for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.83       165\n",
      "           1       0.74      0.73      0.73       103\n",
      "\n",
      "    accuracy                           0.79       268\n",
      "   macro avg       0.78      0.78      0.78       268\n",
      "weighted avg       0.79      0.79      0.79       268\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=0.8)\n",
    "lr.fit(X_train, y_train)\n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86       165\n",
      "           1       0.81      0.73      0.77       103\n",
      "\n",
      "    accuracy                           0.83       268\n",
      "   macro avg       0.82      0.81      0.82       268\n",
      "weighted avg       0.83      0.83      0.83       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 280)\n",
    "rf.fit(X_train, y_train)\n",
    "print(classification_report(y_test, rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.86       165\n",
      "           1       0.79      0.71      0.75       103\n",
      "\n",
      "    accuracy                           0.82       268\n",
      "   macro avg       0.81      0.80      0.80       268\n",
      "weighted avg       0.82      0.82      0.81       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(n_estimators = 280)\n",
    "gb.fit(X_train, y_train)\n",
    "print(classification_report(y_test, gb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "Test set predictions are based on **averaged probabilities between those 3 models**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "gb.fit(X, y)\n",
    "rf.fit(X, y)\n",
    "lr.fit(X, y)\n",
    "print('Training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 class probabilities:\n",
      "[[0.94126077 0.05873923]\n",
      " [0.69797151 0.30202849]\n",
      " [0.81602183 0.18397817]\n",
      " [0.91087081 0.08912919]\n",
      " [0.24660332 0.75339668]]\n"
     ]
    }
   ],
   "source": [
    "proba_gb = gb.predict_proba(X_pred)\n",
    "proba_rf = rf.predict_proba(X_pred)\n",
    "proba_lr = lr.predict_proba(X_pred)\n",
    "proba_avg = (proba_gb + proba_rf + proba_lr) / 3\n",
    "print('First 5 class probabilities:')\n",
    "print(proba_avg[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target class ratio:\n",
      "0    0.636364\n",
      "1    0.363636\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "for proba in proba_avg:\n",
    "    if proba[0] > 0.5:\n",
    "        y_pred.append(int(0))\n",
    "    else:\n",
    "        y_pred.append(int(1))\n",
    "print('Target class ratio:')\n",
    "print(pd.Series(y_pred).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/titanic/gender_submission.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-7c439de98212>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msubmission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/titanic/gender_submission.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSurvived\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PassengerId'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1043\u001b[0m             )\n\u001b[0;32m   1044\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1045\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1860\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1861\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1863\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1864\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1355\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m         \"\"\"\n\u001b[1;32m-> 1357\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1358\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python38\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/titanic/gender_submission.csv'"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('gender_submission.csv')\n",
    "print(submission.head())\n",
    "submission.Survived = y_pred\n",
    "submission.set_index('PassengerId', inplace=True)\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')\n",
    "print('Submission saved successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTES:**\n",
    "1. This is **based on my limited knowledge**.\n",
    "2. I may have reduce some information by binning Age but so far the results are better by doing so.\n",
    "3. I have removed Cabin and Ticket features entirely."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
